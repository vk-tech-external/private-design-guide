# {heading(Роли серверов)[id=arch_server_roles]}

Все серверы, на которые производится установка {var(sys2)}, делятся на два типа:

* Узлы управления (Control Plane).
* Узлы рабочих нагрузок (Data Plane).

В рамках каждого такого слоя серверы выполняют специфические роли, которые предъявляют различные требования к их производительности, количеству и правилам масштабирования.

## {heading(Узлы управления)[id=arch_control_nodes]}

На серверах, относящихся к слою управления, выполняются службы, отвечающие за управление компонентами {var(sys2)} или обеспечивающие их функционирование. Часть ролей слоя управления является опциональной или может быть заменена на оборудование, имеющееся в наличии у Заказчика.

### {heading(Обязательные узлы управления)[id=arch_required_control_nodes]}

#### {heading(Контроллеры)[id=arch_controllers]}

На управляющих узлах (контроллерах) размещаются основные службы {var(sys2)} и их API (Nova, Cinder, Glance и т. д.). Также на серверах с этой ролью размещаются служебные компоненты (служебный кластер Kubernetes, RabbitMQ, Kafka, MariaDB и т.д). Если в документации для компонента не указано иное, компоненты размещаются в служебном кластере Kubernetes.

В крупных инсталляциях допустимо выделять контроллеры для расширения служебного кластера Kubernetes и/или кластеров служебных БД. Рекомендованная конфигурация дополнительных контроллеров соответствует основной конфигурации.

Минимальное количество контроллеров в инсталляции — 3, увеличение количества контроллеров производится согласно разделу {linkto(../../design_principles_main/design_control_loop_server_calculations#calculations_controller_scaling_principles)[text=%text]}.

### {heading(Служебные узлы управления)[id=arch_service_control_nodes]}

#### {heading(Узлы мониторинга и логирования)[id=arch_monitoring_and_logging_nodes]}

На узлах с этой ролью размещаются сервисы мониторинга и логирования.

Мониторинг реализован на базе Zabbix с использованием MySQL для хранения данных.

Логирование реализовано на связке OpenSearch или OpenSearch Dashboard с Apache Kafka и Filebeat.

Особенностью серверов этой роли является высокая зависимость нагрузки от объёма и состава {var(sys2)}, а также интенсивности управляющих воздействий в {var(sys3)} (создание/удаление сущностей).

Для размещения этой роли допустимо использовать виртуальные машины.

По умолчанию рекомендуется закладывать 1 сервер. При большом размере инсталляции может потребоваться увеличение узлов до трёх.

#### {heading(Узел развёртывания/репозитория)[id=arch_deployment_repository_node]}

Эта роль представлена единственным сервером и используется начиная с самых начальных этапов развёртывания {var(sys2)}. В отличие от остальных ролей, этот сервер должен предоставляться Заказчиком с уже установленной ОС.

Основные задачи деплой-сервера:

* Запуск механизмов развёртывания {var(sys2)}.
* Изменение настроек, обновление и восстановление {var(sys2)}.
* Подготовка серверов к включению в состав {var(sys2)}.
* Размещение сервиса репозитория пакетов и образов компонентов {var(sys2)}.

<err>

Деплой-ноду нельзя отключать после развёртывания {var(sys2)}.

</err>

Требуется только 1 сервер. Допустимо дублирование роли. Необходимо выполнять регулярное резервное копирование деплой-ноды с периодическим контролем корректности его восстановления.

## {heading(Узлы рабочих нагрузок)[id=arch_workload_nodes]}

На серверах, относящихся к слою рабочих нагрузок (Data Plane), выполняются компоненты, отвечающие за выполнение рабочих нагрузок и непосредственно рабочая нагрузка.

### {heading(Вычислительные узлы)[id=arch_compute_nodes]}

Вычислительные узлы (Compute Nodes, далее — ВУ) — основные серверы, которые используются для запуска рабочих нагрузок пользователей. Эти узлы поставляются с гипервизором KVM.

Рекомендованные конфигурации отличаются высокой вариативностью в зависимости от требований Заказчика. Возможно применение различных конфигураций в рамках роли с объединением одинаковых серверов в агрегаты (подробнее — в разделе {linkto(../../design_principles_main/design_separation_methods#aggregates_of_computing_nodes)[text=%text]}).

Общая рекомендация: использовать процессоры с максимальным количеством ядер при заданной частоте. Объём памяти зависит от требований к максимальному объёму памяти в типе ВМ и допустимых коэффициентов переподписки.

В инсталляции {var(sys2)} должен быть минимум 1 узел. Чтобы полностью использовать функциональность {var(sys2)} («живую» миграцию, Affinity и Anti-Affinity) необходимо минимум 3 узла. Допустимо увеличивать количество вычислительных узлов по одному серверу.

### {heading(Сетевые узлы)[id=arch_network_nodes]}

На сетевых узлах (Network nodes) размещаются компоненты, реализующие основные сетевые функции (SNAT, DHCP, DNS). Эти узлы можно комбинировать с узлами контроллера.

Минимальное количество сетевых узлов — 2, далее возможно добавлять по 2 узла. Рекомендуемое количество: по одному узлу на каждые 150 виртуальных маршрутизаторов, выполняющих SNAT, и 300 сетей.

### {heading(Узлы Ceph)[id=arch_ceph_nodes]}

На узлах с ролью Ceph устанавливается программно-определяемое хранилище Ceph. {var(sys1)} использует Ceph версии Octopus. В качестве накопителей используются твердотельные накопители.

Минимальное количество узлов в кластере — 3. Кластер масштабируется добавлением узлов кратно трём. Максимальное количество узлов в кластере — 9. Если требуется большая ёмкость дискового хранилища, формируется несколько кластеров Ceph.

### {heading(High-IOPS Storage)[id=arch_high-iops_storage]}

Узлы High-IOPS Storage — это хранилище, реализованное на базе выделенных физических серверов стандартной архитектуры, функционирующих как одиночные экземпляры блочного хранилища.

Высокая производительность и минимальные задержки операций ввода-вывода обеспечиваются за счёт использования NVMe-дисков. Данный вид хранилища не обеспечивает механизмов защиты данных от выхода из строя серверного узла целиком. При этом, с помощью механизмов «горячего» резервирования реализуется защита функционирования сервиса на уровне базовых аппаратных компонентов (электропитание, подключение в сети передачи данных и т. д.). Для защиты пользовательских данных используется механизм репликации блочных данных с помощью технологии LVM Mirror.

<err>

В случае выхода из строя сервера, данные будут недоступны до его восстановления. В случае отказа более двух дисков, в зеркале возможна потеря данных.

</err>

Количество узлов High-IOPS Storage в инсталляции может быть произвольным.