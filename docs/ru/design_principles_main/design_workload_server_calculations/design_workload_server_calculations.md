# {heading(Расчеты серверов рабочих нагрузок)[id=design_workload_server_calculations]}

## {heading(Расчеты количества вычислительных узлов)[id=calculations_of_the_number_of_compute_nodes]}

Количество ВУ напрямую зависит от требований к емкости {var(sys2)} по vCPU и RAM.

При расчете количества ВУ необходимо учитывать, что для функционирования ОС требуется около 10% от имеющегося количества физических ядер и 16 ГБ памяти.

<err>

При расчете требуемых ресурсов CPU необходимо оперировать количеством физических ядер, а не потоков.

</err>

{var(sys1)} позволяет размещать ВМ с переподпиской (overcommit) процессоров. Это позволит увеличить количество экземпляров ВМ, которое можно разместить на {var(sys3)} за счет снижения производительности.

<err>

Переподписка по памяти не предлагается и не поддерживается {var(sys5)}.

</err>

Коэффициент переподписки определяется как отношение требуемого количества виртуального ресурса к реальному количеству физического ресурса:

{formula(O_{CPU} = \frac {Q_{uCPU}}{Q_{CPU}})}

Рекомендованные значения коэффициентов переподписки приведены в {linkto(#tab_oversubscription_rates)[text=таблице %number]}.

{caption(Таблица {counter(table)[id=numb_tab_oversubscription_rates]} — Коэффициенты переподписки)[align=right;position=above;id=tab_oversubscription_rates;number={const(numb_tab_oversubscription_rates)}]}
[cols="3,1", options="header"]
|===
|Коэффициент
|Значение

|Максимальное рекомендованное значение
|8

|Для большинства случаев
|3-5

|Для высоконагруженных приложений
|1
|===
{/caption}

<warn>

Коэффициенты должны быть согласованы с Заказчиком, до него должен быть донесен их смысл и возникающие ограничения.

</warn>

Для расчетов потребуется следующая информация:

* {formula(Q_{req})} — требуемое количество vCPU.
* {formula(Q_{CPU})} — количество физических ядер в одном сервере.
* {formula(O_{CPU})} — коэффициент переподписки по vCPU.
* {formula(E_{CPU})} — коэффициент использования по vCPU. Не может быть выше 0,9.
* {formula(V_{req})} — требуемый объем памяти, ГБ.
* {formula(V_{RAM})} — объем памяти в одном сервере, ГБ.
* {formula(E_{RAM})} — коэффициент использования по RAM. Может быть равным {formula(E_{CPU})}.

Расчет требуемого количества ВУ выполняется по двум следующим формулам:

* Количество серверов по vCPU:

   {formula(Q_{sCPU} = \frac {Q_{reg}}{Q_{CPU} \ast O_{CPU} \ast E_{CPU}})}

* Количество серверов по RAM:

   {formula(Q_{sRAM} = \frac {V_{reg}}{ ( V_{RAM} - 16 ) \ast E_{RAM} })}

Полученные значения округляются вверх до целого. Из двух полученных значений {formula(Q_{sCPU})} и {formula(Q_{sRAM})} рекомендуется выбрать большее.

Дополнительными факторами, влияющими на расчеты, являются:

* Выбранные методы разделения {var(sys2)}.
* Максимальный тип ВМ.
* Требования к Affinity/Anti-Affinity при размещении компонентов.
* Необходимость обеспечения резерва ВУ для планового обслуживания или входа из строя части серверов.

Регион, как метод сегментации, не оказывает прямого влияния на расчет количества ВУ. В большей степени на расчеты оказывают влияние зоны доступности и агрегаты ВУ. Недостаточно просто разделить необходимое количество ВУ на количество зон доступности или количество ВУ в агрегате на количество зон доступности.

Например, для приложения, требующего высокопроизводительные процессоры и большой объем памяти, достаточно 2 ВУ при заполнении 90%. Приложение имеет 4 компонента, которые необходимо размещать на различных узлах. Очевидно, что потребуется минимум 4 ВУ, а с учетом необходимости резервирования ВУ их количество может вырасти до пяти или даже шести.

Таким образом при выполнении расчетов необходимо руководствоваться следующими соображениями:

* Для каждого планируемого агрегата в каждой зоне доступности необходимо закладывать резерв ВУ.
* Расчеты необходимо выполнять для каждого агрегата ВУ в каждой зоне доступности.
* Для агрегатов, где планируется размещать приложения, не предъявляющие высоких требований к производительности, можно применять коэффициент переподписки {formula(O_{CPU})}.
* Необходимый резерв мощностей можно закладывать коэффициентами {formula(E_{CPU})} и {formula(E_{RAM})}.

## {heading(Расчет количества сетевых узлов)[id=calculation_number_network_nodes]}

Сетевые узлы на выделенных серверах могут понадобиться, если в инсталляции планируется использовать большое количество (более 150) проектных (тенантных) сетей и не используется коммутационная фабрика.

Суммарное количество сетевых узлов рассчитывается по формуле:

{formula(Q_N = \frac {N}{150}+1)}

Здесь {formula(N)} — требуемое количество проектных сетей. Получившееся количество округляется вверх до ближайшего целого, кратного двум.

В случае разделения {var(sys2)} на несколько зон доступности необходимо придерживаться следующих правил:

* Сетевые узлы должны быть равномерно распределены по зонам доступности.
* При масштабировании необходимо добавлять узлы во все зоны доступности.

Базовая конфигурация сетевого узла приведена в {linkto(#tab_basic_network_node_configuration)[text=таблице %number]}.

{caption(Таблица {counter(table)[id=numb_tab_basic_network_node_configuration]} — Базовая конфигурация сетевого узла)[align=right;position=above;id=tab_basic_network_node_configuration;number={const(numb_tab_basic_network_node_configuration)}]}
[cols="1,3", options="header"]
|===
|Параметр
|Минимальные требования

|Процессор
|2x Intel Xeon Gold (минимум 28 ядер)

|Оперативная память
|512 ГБ

|Хранилище
|2x 480 ГБ SSD — для ОС

|Сеть
|2x 25 Гбит/с (LACP).

Выделенный интерфейс управления (IPMI, BMC, iDRAC, iLO и т.п.)
|===
{/caption}

## {heading(Расчеты для Ceph)[id=calculations_for_ceph]}

При планировании оборудования Ceph необходимо выполнить балансирование ряда концепций, включающие в себя домены отказа и потенциальные проблемы с производительностью. Не существует «золотого» правила для выбора оборудования для развертывания Ceph, однако необходимо руководствоваться следующими принципами:

* Хранилище необходимо разбивать на небольшие кластеры — 3, 6 или 9 узлов в каждом.
* Ceph предъявляет высокие требования к внутрикластерной сети Ceph Internal. Объем трафика внутрикластерной сети может превышать объем трафика сети доступа к кластеру (Ceph External) в 2–3 раза.
* Не следует допускать заполнение кластера более чем на 70% — это приведет к снижению производительности. При заполнении кластера на 80%, Ceph практически перестает работать.
* Не используется RAID. Диски презентуются в Ceph индивидуально.
* Используйте разбиение NVMe-дисков на 2 раздела.
* Масштабирование хранилища необходимо выполнять путем увеличения количества кластеров. Не рекомендуется изменять размер кластера.
* Размещение OSD с данными монитора, менеджера и MDS на одном диске недопустимо.

Расчеты для Ceph сводятся к расчету необходимого количества ядер CPU и объему RAM в одном сервере и расчету необходимого количества кластеров Ceph.

Для расчета требуемых параметров сервера по CPU и RAM потребуется следующая информация:

* {formula(Q_D)} — количество дисков в одном сервере. Влияет на итоговое количество OSD в сервере.
* {formula(Q_{OSD})} — количество OSD на диске, определяется типом диска:

   * SSD — 1 OSD.
   * NVMe — 2 OSD.

* {formula(T_D)} — пропускная способность диска. Определяет требования к сетевым интерфейсам сети Ceph Internal.

Расчет параметров сервера выполняется по следующим формулам:

* Пропускная способность массива дисков сервера:

   {formula(T_{SD} = {Q_D \ast T_D})}

* Итоговое количество OSD на сервер:

   {formula(Q_{sOSD} = {Q_{OSD} \ast Q_D})}

* Количество ядер CPU на сервер:

   {formula(Q_{CPU} = {Q_{sOSD} * Q_{cores} + Q_{MON} + Q_{MDS} + Q_{OS}})}

   Здесь:

   * {formula(Q_{cores})} — количество ядер на один OSD: минимум 1 ядро (2 потока) для SSD-диска, минимум 2 ядра (4 потока) для NVMe-диска.
   * {formula(Q_{MON})} — количество ядер под ceph-mon (минимум 2 ядра).
   * {formula(Q_{MDS})} — количество ядер под ceph-mds (минимум 2 ядра).
   * {formula(Q_{OS})} — количество ядер под ОС (минимум 8 ядер).

* Объем оперативной памяти на сервер:

   {formula(V_{RAM} = {Q_{sOSD} * V_{OSD} + V_{MON} + V_{MDS} + V_{OS}})}

   Здесь:

   * {formula(V_{OSD})} — объем памяти под ceph-osd (минимум 4 ГБ).
   * {formula(V_{MON})} — объем памяти под ceph-mon (минимум 5 ГБ).
   * {formula(V_{MDS})} — объем памяти под ceph-mds (минимум 2 ГБ).
   * {formula(V_{OS})} — объем памяти под ОС (минимум 4 ГБ).

* Пропускная способность сети Internal:

   {formula(T_{int} \approx U * \frac {N \ast T_N \ast 10^9}{2^{10}})}

   Здесь:

   * {formula(U)} — коэффициент утилизации (обычно устанавливается в диапазоне 0,85–0,9).
   * {formula(N)} — количество сетевых карт для сети Ceph Internal.

Для расчетов количества серверов/кластеров понадобятся следующие входные данные:

* {formula(V_{req})} — требуемый полезный объем хранилища.
* {formula(RF)} — фактор репликации. {var(sys1)} устанавливается с предустановленным фактором репликации 3.
* {formula(E)} — коэффициент максимального заполнения хранилища. Не рекомендуется устанавливать выше 0.7.
* {formula(V_D)} — объем диска (TiB):

  {formula(V_D = \frac {u_D \ast 10^{12}}{2^{40}})} 

  Здесь {formula(u_D)} — объем диска в ТБ.

Расчет требуемого количества серверов выполняется по формуле:

{formula(N_{sru} = \frac {V_{reg} * RF}{Q_D \ast V_D \ast E})}

Полученное число округляется вверх до целого, кратного трем.

На определение количества серверов в кластере влияют следующие факторы:

* Количество серверов в одном кластере должно быть кратно 3, максимальное количество серверов в одном кластере — 9.
* Количество OSD в одном кластере не должно превышать 150.

Например, получено число серверов {formula(N_{srv})} — 11, на каждом сервере 16 OSD. При округлении вверх до кратного трем получаем 12 серверов. Доступны следующие варианты разбиения массива на кластеры:

1. Первый кластер — 9 узлов (144 OSD), второй — 3 (48 OSD).
1. Два кластера по 6 узлов (по 96 OSD).

Очевидно, что второй вариант будет лучше по следующим причинам:

1. Равное количество узлов и OSD во всех кластерах.
1. Не будет создан кластер с минимальным количеством узлов.

На расчеты количества серверов и кластеров могут повлиять следующие факторы:

* Не следует совмещать в одном кластере Ceph накопители разной производительности (SSD и NVMe). Необходимо организовывать разные наборы кластеров под разные типы накопителей.
* Не следует использовать разные объемы накопителей в одном кластере. В кластере необходимо использовать накопители одинаковой емкости.
* Следует стремиться к одинаковому количеству узлов в кластерах для каждого типа накопителей.
* Следует избегать вариантов с большим количеством NVMe дисков в одном сервере, так как это ограничивает количество узлов в кластере. Оптимальным количеством NVMe накопителей для OSD в одном сервере — 8.

## {heading(Расчеты для High-IOPS)[id=calculations_for_high_iops]}

При планировании оборудования High-IOPS Storage следует руководствоваться следующими факторами:

* Масштабирование хранилища необходимо выполнять путем увеличения количества серверов.
* Не рекомендуется допускать заполнения хранилища более чем на 80%.

Для выполнения расчетов потребуется следующая информация:

* {formula(V_{req})} — требуемый полезный объем хранилища.
* {formula(RF)} — фактор репликации, для High-IOPS, всегда равен 2.
* {formula(E)} — коэффициент максимального заполнения хранилища. Не рекомендуется устанавливать выше 0.8.
* {formula(T_D)} — пропускная способность диска.
* {formula(V_D)} — объем диска (TiB):

  {formula(V_D = \frac {u_D \ast 10^{12}}{2^{40}})}

  Здесь {formula(u_D)} — объем диска в ТБ.

Расчеты количества серверов:

* Требуемое количество серверов:

   {formula(N_{sru} = \frac {V_{reg} * RF}{Q_D \ast V_D \ast E})}

* Пропускная способность массива дисков:

   {formula(T_{SD} = \frac {Q_D * T_D}{RF})}

* Минимальное количество ядер CPU на сервер:

   {formula(Q_{CPU} = \frac {T_{SD}}{50000} + Q_{OS})}

   Здесь {formula(Q_{OS})} — количество ядер под ОС (минимум 8 ядер).

* Пропускная способность сети сервера:

   {formula(T_{int} \approx U * \frac {N \ast T_N \ast 10^9}{2^{10}})}

   Здесь:

   * {formula(U)} — коэффициент утилизации (обычно устанавливается в диапазоне 0,85–0,9).
   * {formula(N)} — количество сетевых карт для сети Ceph Internal.

## {heading(Расчеты для Внешних СХД)[id=calculations_for_nvme]}

При планировании подключения внешних СХД важно учитывать следующие аспекты:

* Производительность и надежность работы внешних СХД зависят от производительности и отказоустойчивости внешнего решения для организации СХД. Если эти показатели недостаточны, работа конечных систем с этим СХД будет ограничена.
* Количество LUN может меняться и увеличиваться в зависимости от потребностей конечных систем, однако для начала работы достаточно подключить одну LUN.
* Масштабирование доступного пространства СХД происходит за счет подключения дополнительных LUN к узлам {var(sys2)}.
